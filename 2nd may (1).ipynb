{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b82b27d-3557-4631-8c8d-6a3e14af5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is anomaly detection and what is its purpose?\n",
    " \n",
    "Anomaly detection is a technique used in data analysis to identify patterns in data that deviate significantly from the \n",
    "expected or normal behavior. The purpose of anomaly detection is to detect unusual or anomalous behavior that may indicate\n",
    " a problem, threat, or opportunity.\n",
    "\n",
    "purpose of anomaly detection :\n",
    "\n",
    "i.Anomaly detection can be used in various fields such as cybersecurity, fraud detection, fault detection, predictive maintenance, and more.\n",
    "ii.Anomaly detection techniques can be supervised, unsupervised, or semi-supervised, depending on the availability of labeled data.\n",
    "iii.In supervised anomaly detection, a model is trained on labeled data to classify anomalies, while in unsupervised anomaly detection, \n",
    "the model learns the normal behavior of the data and flags any deviation from it as an anomaly.\n",
    "iv.Anomaly detection can be performed using various statistical, machine learning, and deep learning techniques, such as clustering, nearest\n",
    " neighbor, SVM, neural networks, and more.\n",
    "v.The effectiveness of anomaly detection depends on the quality of data, the choice of algorithm, and the definition of anomalies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a5eb91-8700-43db-b710-73313d7546d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Q2. What are the key challenges in anomaly detection?\n",
    "\n",
    "some of the main challenges:\n",
    "\n",
    "i.Lack of labeled data: Supervised anomaly detection requires labeled data, which can be difficult to obtain in some domains. Collecting\n",
    " and labeling data can be time-consuming and expensive, and in some cases, anomalies may not be well-defined or may be rare, making it \n",
    "challenging to create a representative labeled dataset.\n",
    "\n",
    "ii.Imbalanced data: In many datasets, anomalies are rare compared to normal data, resulting in imbalanced data. This can make it difficult \n",
    "for the model to learn to detect anomalies, as it may be biased towards the majority class.\n",
    "\n",
    "iii.Lack of clear definition of anomalies: In some domains, what constitutes an anomaly may not be well-defined or may be subjective.\n",
    " For example, in fraud detection, the definition of fraud may vary depending on the context or jurisdiction.\n",
    "\n",
    "iv.Concept drift: Data distributions may change over time, making it difficult for models trained on past data to detect anomalies in the\n",
    " current data. This is known as concept drift and requires continuous monitoring and adaptation of anomaly detection models.\n",
    "\n",
    "v.Noisy data: Real-world data is often noisy, containing errors, missing values, and outliers that can impact the effectiveness of anomaly detection algorithms.\n",
    "\n",
    "vi.Scalability: Some anomaly detection algorithms can be computationally expensive and may not scale well to large datasets or real-time processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57245ed-17e7-46b4-8105-bf8155d445f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "\n",
    "the choice between unsupervised and supervised anomaly detection depends on the availability of labeled data and the nature of the problem domain. \n",
    "Supervised anomaly detection can be more effective when labeled data is available, but unsupervised anomaly detection can be useful in cases where \n",
    "labeled data is scarce or non-existent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf6895-1df6-46f7-a28c-e16f904f1360",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are the main categories of anomaly detection algorithms?\n",
    "Ans: the main categories of anomaly detection algorithms\n",
    "i.Isolation forest(Decision tree)\n",
    "ii.DBSCAN clustering\n",
    "iii.Local Outlier factor Anamoly Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d082a5-b3f7-4aea-84e8-1b9324928854",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
    "\n",
    "Distance-based anomaly detection methods rely on the assumption that normal data points are tightly clustered together, while anomalies are \n",
    "located far away from the normal cluster. These methods use a distance or similarity measure to quantify the distance between data points and \n",
    "identify those that are significantly far away from the rest of the data. Here are some of the main assumptions made by distance-based anomaly detection methods:\n",
    "i.Normal data points are tightly clustered together: \n",
    "ii.Anomalies are far away from normal data points: \n",
    "iii.Distance or similarity measure can accurately capture dissimilarity:\n",
    "iv.Data is represented in a meaningful way:\n",
    "distance-based anomaly detection methods can be effective when these assumptions are met, but they may not work well in all scenarios. These methods \n",
    "should be evaluated carefully in the context of the specific problem domain and the characteristics of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf9c0dc-75a3-461d-beb4-8b6d12243ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Q6. How does the LOF algorithm compute anomaly scores?\n",
    "\n",
    "The LOF (Local Outlier Factor) algorithm computes anomaly scores for each data point based on its local density compared to the density of its neighbors.\n",
    " Here are the main steps of the LOF algorithm:\n",
    "i.For each data point, identify its k-nearest neighbors: \n",
    "ii.Compute the reachability distance for each data point: \n",
    "iii.Compute the local reachability density for each data point: \n",
    "iv.Compute the local outlier factor for each data point: \n",
    "v.Interpret the local outlier factors as anomaly scores: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a088f7-484c-4625-a8c1-972dba308fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Q7. What are the key parameters of the Isolation Forest algorithm?\n",
    "The Isolation Forest algorithm is an unsupervised anomaly detection method that uses decision trees to isolate anomalies from normal data points. \n",
    "The algorithm has several key parameters that can be tuned to optimize its performance for a given data set. Here are the main parameters of the Isolation Forest algorithm:\n",
    "\n",
    "i.n_estimators: This parameter determines the number of decision trees to be used in the Isolation Forest. A higher number of trees can increase\n",
    " the detection accuracy but also increase the computational cost.\n",
    "\n",
    "ii.max_samples: This parameter determines the number of data points to be randomly selected as the subsample for each tree in the Isolation Forest.\n",
    " A smaller subsample size can increase the diversity of the trees and improve the detection accuracy but also increase the variance and reduce the stability of the results.\n",
    "\n",
    "iii.max_features: This parameter determines the maximum number of features to be randomly selected as the split candidates for each tree in the Isolation Forest.\n",
    " A smaller number of features can increase the diversity of the trees and improve the detection accuracy but also increase the bias and reduce the discriminative power of the model.\n",
    "\n",
    "iv.contamination: This parameter determines the fraction of anomalies in the data set. This parameter is used to adjust the threshold for anomaly detection and \n",
    " determine the number of data points to be classified as anomalies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c561b00-afca-4aa7-ad60-56fa74279fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Assume X is a 2D numpy array of shape (n_samples, n_features) containing the data points\n",
    "\n",
    "# Calculate pairwise distances between all data points\n",
    "distances = pairwise_distances(X)\n",
    "\n",
    "# Find indices of data points that are within a radius of 0.5\n",
    "indices = np.where(distances < 0.5)\n",
    "\n",
    "# Create a mask to filter out data points that have less than 2 neighbors of the same class\n",
    "mask = np.zeros(X.shape[0], dtype=bool)\n",
    "for i in np.unique(indices[0]):\n",
    "    neighbors = indices[1][indices[0] == i]\n",
    "    if len(set(y[neighbors])) == 1:\n",
    "        mask[i] = True\n",
    "\n",
    "# Filter out data points that have less than 2 neighbors of the same class\n",
    "X_filtered = X[mask]\n",
    "\n",
    "# Fit a KNN model with K=10\n",
    "knn = NearestNeighbors(n_neighbors=10)\n",
    "knn.fit(X_filtered)\n",
    "\n",
    "# Calculate the anomaly score of each data point\n",
    "scores = []\n",
    "for i in range(X.shape[0]):\n",
    "    if not mask[i]:\n",
    "        # If the data point has less than 2 neighbors of the same class within a radius of 0.5,\n",
    "        # set its anomaly score to NaN\n",
    "        scores.append(np.nan)\n",
    "    else:\n",
    "        # Calculate the distance to the 10th nearest neighbor\n",
    "        distances, indices = knn.kneighbors([X[i]])\n",
    "        distance_to_10th_neighbor = distances[0][-1]\n",
    "        # Calculate the anomaly score as the inverse of the distance to the 10th nearest neighbor\n",
    "        score = 1 / distance_to_10th_neighbor\n",
    "        scores.append(score)\n",
    "\n",
    "# Normalize the anomaly scores to have mean 1 and standard deviation 1\n",
    "scores = (scores - np.nanmean(scores)) / np.nanstd(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc136fae-7e0b-437f-ae18-266d74ee96b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q9.Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is \n",
    "the anomaly score for a data point that has an average path length of 5.0 compared to the average path \n",
    "length of the trees?\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Generate a random dataset of 3000 points with 10 features\n",
    "X = np.random.rand(3000, 10)\n",
    "\n",
    "# Fit an Isolation Forest with 100 trees\n",
    "forest = IsolationForest(n_estimators=100, contamination='auto')\n",
    "forest.fit(X)\n",
    "\n",
    "# Generate a test data point with an average path length of 5.0\n",
    "x_test = np.random.rand(1, 10)\n",
    "path_lengths = forest.decision_path(x_test)[0].toarray()[0]\n",
    "avg_path_length = np.mean(path_lengths)\n",
    "\n",
    "# Compute the anomaly score for the test data point\n",
    "c = np.log2(len(X))\n",
    "estimated_path_length = 2**(-5.0 / c)\n",
    "score = 2**(-avg_path_length / c) / estimated_path_length\n",
    "\n",
    "print(\"Anomaly score:\", score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
